\section{A Complete Example}
\label{sec:complete_example}

The modules described in the previous sections of this chapter work together to
provide an adapted user interface for the sensed user's context disabilities. 
The following section presents an example which details the whole process aiming
to demonstrate the adaptation process for a concrete user interface. This 
demonstration is carried out through the introduction of particular scenario. 
This scenario introduces Molly, a user who is trying to make a phone call from 
the beach with her Android mobile device. The characteristics of Molly's context 
cause her problems to interact with the phone and achieve her goal. The main 
identified context issue is the amount of luxes in the current situation, which 
might induce temporary sight disabilities. The details of this scenario are shown
in Table~\ref{tbl:example_scenario}.

\begin{table}[H]
 \caption{Scenario summary.}
 \label{tbl:example_scenario}
 \footnotesize
 \centering
\begin{tabular}{l l}
  \hline 
				& \textbf{Scenario}	\\
  \hline
  User \\
  \qquad - Personal data 	& Molly, 39 years old, United States\\
  \qquad - Activity	 	& Phone call		\\
  \qquad - Known disabilities 	& - 			\\
% 				& Hearing loss 		\\
%   \hline
  Context \\
  \qquad - Location 		& Relative: Plentzia, Spain\\
				& 			\\
  \qquad - Time			& 14:35 		\\
  \qquad - Brightness		& 30,000 \ac{lx}	\\
  \qquad - Noise		& 80 \acp{db}		\\
  \qquad - Temperature		& 28ºC 			\\
%   \hline
  Device 			& Samsung Galaxy SII 	\\
				& Battery: 55\%	\\
  \hline
\end{tabular}
\end{table}

% The following lines detail the adaptation process, highlighting each layer's and
% module's goal and behaviour. As the user is trying to perform a phone call, the 
% corresponding default user interface is shown in Figure~\ref{fig:example_scenario_default}.

To make the phone call, Molly is using the application whose user interface is
shown in Figure~\ref{fig:example_scenario_default}. Thus, the shown user interface
should be adapted by AdaptUI if the scenario presents characteristics that might
trouble Molly during the phone call process.

\begin{figure}[H]
\centering
\includegraphics[width=0.25\textwidth]{example_scenario_default.png}
\caption{A simple phone call application user interface.}
\label{fig:example_scenario_default}
\end{figure}

First, the proper functioning of the AdaptUI platform highly depends on the
interaction capabilities represented in the AdaptUIOnt ontology. These capabilities
are first gathered by the Capabilities Collector. This module also collects context
and device characteristics to build the corresponding semantic model. Hence,
the Capabilities Collector has to be run by Molly in her device. The first task
this module performs is collecting several characteristics of the used device.
This is made by calling several Android system methods which allow developers
know the current capabilities of the device (e.g., available memory and battery).
Next, several activities are presented through which Molly is asked to calibrate
and configure different visual elements, such as buttons, edit texts and text
views. This has to be performed considering the current context situation. 
Otherwise, it would not be useful for future adaptations within a similar context. 
As the context might vary during the capabilities collecting process, the module
uses a native pre-semantic representation of the gathered knowledge. Using a simple
key-value representation each interaction is stored in a SharedPreferences based
model. This allows Molly to specify the needed interaction preferences rapidly.
The process ends with the translation of this model into a semantic one, represented
by the AdaptUIOnt ontology. This takes several seconds, and it is performed by
the Semantic Modeller. 

The Capabilities Collector represents the device's characteristics as Table~\ref{tbl:device}
shows.

\begin{table}
 \caption{The device's characteristics represented by the Capabilities Collector.}
 \label{tbl:device}
 \footnotesize
 \centering
\begin{tabular}{l l l}
  \hline 
  \textbf{Class(*)}& \textbf{Key} & \textbf{Value}		\\
  \hline
  \ac{os}	& \textit{osVersion}		& 4.1.2	\\
  Battery	& \textit{battery}		& 55		\\
  Volume	& \textit{maxVolume}		& 12		\\
  Brightness	& \textit{maxBrightness}	& 15		\\
  DeviceScreenResolution & \textit{resolution}	& 480×800	\\	
  \hline
\end{tabular}
\end{table}

The most significant characteristics collected by the Capabilities Collector
are the battery level and the maximum brightness and volume available levels.
The first one might determine if the adaptation can lead to an excessive power
consumption, while the last two determine the adaptation boundaries for 
brightness and volume.

Next, the activities which collect the interaction capabilities of the user
represent the gathered information as shown in Table~\ref{tbl:user}.

\begin{table}
 \caption{The user's characteristics represented by the Capabilities Collector.
 The colour set (*) represents the whole colour configuration that the user 
 specifies for the corresponding views, including the background colours, text 
 colours and so on.}
 \label{tbl:user}
 \footnotesize
 \centering
\begin{tabular}{l l l}
  \hline 
  \textbf{Class(*)}& \textbf{Key} & \textbf{Value}		\\
  \hline
  Interface 	& \textit{input}		& default	\\
		& \textit{output} 		& default	\\
  View		& \textit{colourSet(*)}		& default	\\
  Display 	& \textit{minBrightness}	& 9		\\ 
 		& \textit{maxBrightness}	& max		\\
  Audio 	& \textit{minVolume}		& 4		\\
 		& \textit{maxVolume} 		& max		\\
  Other 	& \textit{language}		& English	\\
 		& \textit{vibration} 		& true 		\\	
  \hline
\end{tabular}
\end{table}

Due to the context situation Molly specifies a minimum brightness value of 9.
Taking into account that the maximum available value is 15, it represents a
high value within the brightness range. This is due to the context brightness 
that she is suffering, which impedes a proper use of the display. More concretely,
the current amount of luxes (30,000 \ac{lx}) causes trouble when using the
display. The described context situation is detailed in Table~\ref{tbl:context}.

\begin{table}
 \caption{The context's characteristics represented by the Capabilities Collector.}
 \label{tbl:context}
 \footnotesize
 \centering
\begin{tabular}{l l l}
  \hline 
  \textbf{Class(*)}& \textbf{Key} & \textbf{Value}		\\
  \hline
  Brightness	& \textit{brightness}		& 30,000	\\
  Noise		& \textit{noise}		& 80		\\
  Time		& \textit{time}			& 14:35	\\
  Location	& \textit{city}			& Plentzia	\\
		& \textit{country}		& Spain		\\
		& \textit{longitude}		& 43.414353	\\
		& \textit{latitude} 		& -2.944183	\\	
  \hline
\end{tabular}
\end{table}


As mentioned before, in order to allow AdaptUI to properly adapt the user 
interface the user is asked to use the Capabilities Collector. 
Figure~\ref{fig:capabilities_collector_scenario} shows one of the Capabilities
Collector's activities in which Molly is choosing different colours for the
corresponding views.

\begin{figure}
\centering
\includegraphics[width=0.30\textwidth]{capabilities_collector_scenario.pdf}
\caption{A user configuring the buttons colour set through the Capabilities Collector.}
\label{fig:capabilities_collector_scenario}
\end{figure}

Once the interaction with the Capabilities Collector is finished, the Semantic 
Modeller uses the SharedPreferences stored model and stores it as a semantic 
representation of this model in the AdaptUIOnt ontology. The semantic representation
of the knowledge represented in Table~\ref{tbl:user}, Table~\ref{tbl:device} and
Table~\ref{tbl:context} is shown in Table~\ref{tbl:capabilities_collector_scenario}.


\begin{table}
 \caption{User, context and device profiles as represented in the AdaptUIOnt 
 ontology.}
 \label{tbl:capabilities_collector_scenario}
 \footnotesize
 \centering
\begin{tabular}{l l l l}
  \hline 
  \textbf{Entity}& \textbf{Class} & \textbf{Property} 			& \textbf{Value}\\
  \hline
  User 		& Display 	& \textit{userDisplayIsApplicable} 	& true		\\% Esto se saca con reglas
		& 		& \textit{userDisplayBrightnessIsStatic}& false		\\
		& Audio 	& \textit{userDisplayApplicableIsStatic}& false		\\
		& 		& \textit{userAudioHasApplicable} 	& true		\\
		& 		& \textit{userAudioApplicableIsStatic} 	& false		\\
		& 		& \textit{userAudioHasVolume}  		& 4 		\\
		& Interface 	& \textit{userInterfaceInput}		& default	\\
		& 		& \textit{userInterfaceOutput} 		& default	\\
% 		& Experience	& \textit{userHasExperience} 		& high		\\
		& View		& \textit{userViewIsStatic}		& false		\\
		& Other 	& \textit{userHasLanguage}		& English	\\
		& 		& \textit{vibration} 			& true 		\\
  Context	& Brightness	& \textit{contextHasBrightness}		& 30,000	\\
		& Noise		& \textit{contextHasNoise}		& 80		\\
		& Time		& \textit{contextHasTime}		& 14:35	\\
		& Location	& \textit{contextHasRelativeLocationCity}& Plentzia	\\
		&		& \textit{contextHasRelativeLocationCountry}& Spain	\\
		&		& \textit{contextHasAbsoluteLocationLongitude}& 43.414353\\
		&		& \textit{contextHasAbsoluteLocationLatitude} & -2.944183\\
  Device	& Software (\ac{os})& \textit{deviceHasOSVersion}	& 4.1.2	\\
		& Hardware (Battery)& \textit{deviceHasBattery}		& 55		\\
		& (Volume)	& \textit{deviceHasMaxVolume}		& 12		\\
		& (DeviceScreenResolution) & \textit{deviceHasResolution}& 480×800	\\	
  \hline
\end{tabular}
\end{table}


Consequently, the first rules of the AdaptUIOnt ontology are triggered. These 
rules form the pre-adaptation rules set, and their goal is to compute possible 
interaction disabilities for the user tanking into account the input user 
interaction capabilities, context sensed characteristics, and device features. 
For example, Equation~\ref{ec:pre_rule} updates the 
\textit{userAuxDisplayHasApplicable} datatype property by checking if the user 
interacts with the Capabilities Collector in a \textit{default} manner.
    
\footnotesize
\begin{equation} \label{ec:pre_rule} 
\begin{align*} 
UserAux(?uaux) \; \&\; UserCharacteristics(?user) \; \&\; \\
Interface(?interf) \; \&\; Input(?input) \; \&\; \\
userHasInterface(?user, ?interf) \; \&\; interfaceHasInput(?interf, ?input) \; \&\;\\
swrlb:equal(``default") \\    
\Rightarrow \\
userAuxDisplayHasApplicable(?uaux, true)
\end{align*}
\end{equation}
\normalsize

As this is an intermediate and internal process Molly still does not perceive
any adaptation or change. It is in the next step when the adaptation rules
determine the new configuration of the presented views shown in 
Figure~\ref{fig:example_scenario_default}. The Adaptation Engine loads the 
stored AdaptUIOnt model, and it executes the corresponding adaptation rules 
defined by the developer, whose goal is to make the corresponding modifications
on the mentioned views. Once the rules have been executed, the resulting user 
interface is dynamically updated and presented to the user. 
Equation~\ref{ec:adap_rule} shows how the views are updated due to the context 
brightness. The resulting adapted user interface is then presented to Molly, and 
its appearance is shown in Figure~\ref{fig:example_scenario_default_vs_adapted}.

\footnotesize
\begin{equation} \label{ec:adap_rule} 
\begin{align*} 
Button(?b) \; \&\; ContextCharacteristics(?c) \; \&\; gumo:Light(?light) \; \&\; \\  
gumo:PhysicalEnvironment(?pe) \; \&\; contextHasPhysicalEnvironment(?c, ?pe) \; \&\; \\ 
lightHasBrightness(?light, ?value) \; \&\; physicalEnvironmentHasLight(?pe, ?light) \; \&\; \\
swrlb:greaterThanOrEqual(?value, 20000) \\
\Rightarrow \\
buttonHasBackgroundColor(?b, -16711936) %%This should be the Adaption class
\end{align*}
\end{equation}
\normalsize

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{example_scenario_default_vs_adapted.png}
\caption{The default user interface (left) and the adapted user interface (right).}
\label{fig:example_scenario_default_vs_adapted}
\end{figure}


Now the user interface has been adapted there are two possibilities. On the one
hand, if Molly is satisfied with the provided adaptation, AdaptUI finishes its
process. On the other hand, there is the possibility of rejection. In any case,
the system checks the satisfaction of the user by computing several usability
metrics which determine the usability comfort with the presented user interface.
The Adaptation Polisher builds and compares two interaction models. The first 
one is built with the usability metrics detailed in 
Section~\ref{sec:usability_metrics} under default circumstances. This means that 
the model is built when Molly interacts with the device in a scenario in which
there are no context disabilities. On the contrary, the second model, is built 
from the same usability metrics but taking into account the adapted user interface. 
The Interaction Model Comparator evaluate the usability metrics differences
between the two interaction models. Once these rules are executed, their results 
update the last adaptation of the user interface if needed. The polished user 
interface is compared with the adapted one in 
Figure~\ref{fig:example_scenario_adapted_vs_polished}. In the presented scenario, 
the Equation~\ref{ec:post_rule} updates polishes the button's text as follows: 

\footnotesize
\begin{equation} \label{ec:post_rule} 
\begin{align*} 
Button(?b) \; \&\; Usability (?u) \; \&\; EfficiencyMetric(?em) \; \&\; \\
usabilityHasEfficiencyMetric(?u, ?em) \; \&\; swrlb:greaterThanOrEqual(?value, 0.5) \; \&\; \\
efficiencyMetricHasTaskEffectiveness(?em, ?value)\\
\Rightarrow \\
buttonHasTextColor(?b, 16777215)
\end{align*}
\end{equation}
\normalsize

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{example_scenario_adapted_vs_polished.png}
\caption{The adapted user interface (left) and the polished user interface (right).}
\label{fig:example_scenario_adapted_vs_polished}
\end{figure}


As this example demonstrates, the polisher rules allow the developer to keep
improving the adapted user interfaces. This is carried out by maintaining both
default and adapted interaction models which are based on the usability metrics
detailed in Section~\ref{sec:usability_metrics}.

% Modelling Layer
% - Capabilities Collector
% 1. Llegan las capacidades.
% 2. Se genera el modelo.
% 3. Se guarda en formato SharedPreferences.
% - Semantic Layer
% 4. Se guarda en formato semántico.
% 
% Adaptation Layer
% - Adaptation Engine
% 1. Se analiza el modelo semántico.
% 2. Se ejecutan las reglas correspondientes.
% 3. Se actualiza la interfaz con la nueva adaptación resultante.
% - Adaptation Polisher
% 1. Se genera el modelo de interacción.
% 2. Se ejecutan las reglas de interacción.
% 3. Se envían los resultados al Polishing instructions Generator.
% 4. Se ejecutan las reglas de polishing.
% 5. Se envía al Adaptation Engine la interfaz a adaptar.
